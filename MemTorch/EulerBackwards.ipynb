{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make boxes window width. \n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from torch.autograd.functional import vjp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Block, self).__init__()\n",
    "    def forward(self, x, t, net_params):\n",
    "        size = x.size()\n",
    "        x = F.relu(F.conv2d(x.view(-1,1,28,28), net_params[0:9].view(1,1,3,3), padding=1))\n",
    "        x = F.relu(F.conv2d(x, net_params[9:18].view(1,1,3,3), padding=1))\n",
    "        return x.view(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(*args):\n",
    "    print(\"flattening\")\n",
    "    return(torch.cat(tuple(torch.flatten(arg) for arg in args), dim=0).view(1,-1))\n",
    "\n",
    "def unflatten(x, n_e, sizes):\n",
    "    print(\"unflattening\")\n",
    "    return (x[0, 0:n_e[0]].view(sizes[0]),\n",
    "            x[0, n_e[0]:n_e[0] + n_e[1]].view(sizes[1]),\n",
    "            x[0, n_e[0] + n_e[1]:n_e[0] + n_e[1] + n_e[2]].view(sizes[2]),\n",
    "            x[0, n_e[0] + n_e[1] + n_e[2]:].view(sizes[3]),\n",
    "            )\n",
    "\n",
    "class Integrate(torch.autograd.Function):\n",
    "    def __deepcopy__(self, memo):\n",
    "        return Integrate(copy.deepcopy(memo))\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, Integrator, f, x0, t0, t1, N, net_params):\n",
    "        \n",
    "        # Forward, Runge-Kutta 4th Order. \n",
    "        \n",
    "        # Forward integration\n",
    "        with torch.enable_grad(): #necessary for gradient calculations\n",
    "            solution = Integrator(f, x0, t0, t1, N, net_params)\n",
    "            \n",
    "        # Save for jacobian calculations in backward()\n",
    "        ctx.save_for_backward(x0,t0,t1)\n",
    "        ctx.net_params = net_params\n",
    "        ctx.solution = solution\n",
    "        ctx.Integrator = Integrator\n",
    "        ctx.N = N\n",
    "        ctx.f = f\n",
    "        \n",
    "        return solution\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, dL_dz1):\n",
    "        print(\"backward\")\n",
    "        # Get all saved context\n",
    "        z0, t0, t1 = ctx.saved_tensors\n",
    "        net_params = ctx.net_params\n",
    "        z1 = ctx.solution\n",
    "        \n",
    "        N = ctx.N\n",
    "        f = ctx.f\n",
    "        \n",
    "        # Convenience sizes\n",
    "        batch_size = z0.size()[0]\n",
    "        print(z0.size())\n",
    "        img_len = int(z0.numel() / batch_size)\n",
    "\n",
    "        # Compute derivative w.r.t. to end time of integration\n",
    "        dL_dt1 = dL_dz1.view(batch_size,1,-1).bmm(f(z1, t1, net_params).view(batch_size,-1,1))  # Derivative of loss w.r.t t1\n",
    "        \n",
    "        # Initial Condition\n",
    "        num_elements = (z1.numel(), dL_dz1.numel(), batch_size * net_params.numel(), dL_dt1.numel())\n",
    "        sizes = (z1.size(), dL_dz1.size(), (batch_size, net_params.numel()), dL_dt1.size())\n",
    "        \n",
    "        s0 = flatten(z1, dL_dz1, torch.zeros((batch_size, net_params.numel()), dtype=torch.float32), -dL_dt1) # initial augmented state\n",
    "        \n",
    "        print(\"s0\", s0.size())\n",
    "        \n",
    "        # augmented dynamics function\n",
    "        # what I really want is a Tensorflow Ragged Tensor, and pytorch's implementation really isn't there yet\n",
    "        def aug_dynamics(s, t, theta):\n",
    "            print(\"aug_dynamics\")\n",
    "            s = unflatten(s, num_elements, sizes)\n",
    "            with torch.enable_grad(): \n",
    "                gradients = [vjp(f, \n",
    "                                 (s[0][i].unsqueeze(0), t, theta), \n",
    "                                  v=-s[1][i].unsqueeze(0)\n",
    "                                 )[1] for i in range(batch_size)]\n",
    "            y = flatten(f(s[0],t,theta),\n",
    "                    torch.cat([gradient[0] for gradient in gradients], dim=0), \n",
    "                    torch.cat([gradient[2].reshape(1,18) for gradient in gradients], dim=0), \n",
    "                    torch.cat([gradient[1].reshape(1,1) for gradient in gradients], dim=0),\n",
    "                   ).unsqueeze(2)\n",
    "            print(\"finished aug_dynamics\")   \n",
    "            return y\n",
    "        \n",
    "        print(\"integrating backwards dynamics\")\n",
    "        # Integrate backwards\n",
    "        print(s0.size())\n",
    "        with torch.no_grad(): back_dynamics = ctx.Integrator(aug_dynamics, s0, t1, t0, N, net_params)\n",
    "        \n",
    "        # Extract derivatives\n",
    "        _, dL_dz0, dL_dtheta, dL_dt0 = back_dynamics[-1]\n",
    "\n",
    "        # must return something for every input to forward, None for non-tensors\n",
    "        return None, None, None, dL_dz0, dL_dt0, dL_dt1, None, dL_dtheta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODENet(nn.Module):\n",
    "    def __init__(self, solver, f, solver_params):\n",
    "        super(ODENet, self).__init__()\n",
    "        \n",
    "        self.f = f()\n",
    "        \n",
    "        self.int_f = solver\n",
    "        self.Integrate = Integrate()\n",
    "        \n",
    "        self.solver_params = solver_params\n",
    "        self.N = solver_params[\"N\"]\n",
    "        self.h = (solver_params[\"t1\"] - solver_params[\"t0\"]) / solver_params[\"N\"]\n",
    "        self.t0 = torch.tensor(float(solver_params[\"t0\"]), requires_grad=True)\n",
    "        self.t1 = torch.tensor(float(solver_params[\"t1\"]), requires_grad=True)\n",
    "        self.net_params = torch.nn.parameter.Parameter(torch.Tensor(18).normal_(mean=0, std=0.1,generator=None), requires_grad=True)\n",
    "\n",
    "        self.avg_pool = torch.nn.MaxPool2d(2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(196, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.Integrate.apply(self.int_f, self.f, to_v(x), self.t0, self.t1, self.N, self.net_params)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(-1, 196) \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_loader, test_loader, hyperparameters):\n",
    "    \n",
    "    lr = hyperparameters[\"lr\"]\n",
    "    n_epochs = hyperparameters[\"n_epochs\"]\n",
    "    momentum = hyperparameters[\"momentum\"]\n",
    "    \n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n",
    "    \n",
    "    losses = []\n",
    "    for i in range(n_epochs):\n",
    "        \n",
    "        # Train\n",
    "        net.train()\n",
    "        train_losses = []\n",
    "        for j, (data, label) in enumerate(train_loader):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            data.requires_grad = True\n",
    "            output = net(data)\n",
    "            print(\"output\", output.size())\n",
    "            loss = F.nll_loss(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        \"\"\" Check ur gradients if the accuracy sucks\n",
    "        for name, param in net.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(name, param.data, param.grad)\n",
    "        \"\"\"\n",
    "        \n",
    "        num_correct, test_losses = test(net, test_loader)\n",
    "        losses.append([train_losses, test_losses])\n",
    "        \n",
    "        # Report\n",
    "        print(\n",
    "          \"Avg Train Loss\", sum(train_losses)/len(train_losses), \"\\n\"\n",
    "          \"Avg Test Loss\", sum(test_losses)/len(test_losses), \"\\n\"\n",
    "          \"Test Accuracy\", (num_correct / float(len(test_loader.dataset)) * 100).item(), \"%\"\n",
    "         )\n",
    "        print(\"----------------------------------------\")\n",
    "        \n",
    "    return losses    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, test_loader):\n",
    "    # Test\n",
    "    net.eval()\n",
    "    test_losses = []\n",
    "    num_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for j, (data, label) in enumerate(test_loader):\n",
    "\n",
    "            output = net(data)\n",
    "            loss = F.nll_loss(output, label)\n",
    "            test_losses.append(loss.item())\n",
    "            num_correct += label.eq(torch.max(output, 1, keepdim=False, out=None).indices).sum()\n",
    "\n",
    "    \n",
    "    return num_correct, test_losses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 1\n",
    "batch_size_test = 1000\n",
    "img_size = 28\n",
    "img_len = 784\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/Users/louis/Desktop/neuralODE/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/Users/louis/Desktop/neuralODE/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch.autograd.functional import jacobian\n",
    "from torch import inverse as inv\n",
    "from torch import bmm\n",
    "from torch.autograd import grad\n",
    "\n",
    "def jacobian(f, x):\n",
    "    \"\"\"Computes the Jacobian of f w.r.t x.\n",
    "\n",
    "    This is according to the reverse mode autodiff rule,\n",
    "\n",
    "    sum_i v^b_i dy^b_i / dx^b_j = sum_i x^b_j R_ji v^b_i,\n",
    "\n",
    "    where:\n",
    "    - b is the batch index from 0 to B - 1\n",
    "    - i, j are the vector indices from 0 to N-1\n",
    "    - v^b_i is a \"test vector\", which is set to 1 column-wise to obtain the correct\n",
    "        column vectors out ot the above expression.\n",
    "\n",
    "    :param f: function R^N -> R^N\n",
    "    :param x: torch.tensor of shape [B, N]\n",
    "    :return: Jacobian matrix (torch.tensor) of shape [B, N, N]\n",
    "    \"\"\"\n",
    "\n",
    "    B, N = x.shape\n",
    "    y = f(x)\n",
    "    jacobian = list()\n",
    "    for i in range(N):\n",
    "        v = torch.zeros_like(y)\n",
    "        v[:, i] = 1.\n",
    "        dy_i_dx = grad(y,\n",
    "                       x,\n",
    "                       grad_outputs=v,\n",
    "                       retain_graph=True,\n",
    "                       create_graph=True,\n",
    "                       allow_unused=True)[0]  # shape [B, N]\n",
    "        jacobian.append(dy_i_dx)\n",
    "\n",
    "    jacobian = torch.stack(jacobian, dim=2).requires_grad_()\n",
    "\n",
    "    return jacobian\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Batched gradient wrapper for newton().\n",
    "    Input f is a function\n",
    "    x should be a 2-D tensor, (batch, -1), use lambda in f to reshape if necessary\n",
    "    output of f() should be same dimension as x. \n",
    "\"\"\"  \n",
    "\n",
    "\n",
    "\n",
    "def b_grad(f, x, x0):\n",
    "    print(\"b_grad\")\n",
    "    j_list = []\n",
    "    for j in range(x.size()[0]):\n",
    "        with torch.enable_grad():\n",
    "            print(j)\n",
    "            j_list += [torch.autograd.functional.jacobian(lambda x: f(x, x0[j:j+1]), x[j:j+1]).view(1, x.size()[1], x.size()[1])]\n",
    "    return(torch.cat(j_list, dim=0)) \n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "def b_grad(f, x):\n",
    "    return jacobian(lambda x: f(x), x.view(x.size()[0], -1))\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Newton's method for functions f: batch x R^n -> batch x R^n\n",
    "    Input f is a function\n",
    "    x0 is the initial guess, 2-D tensor, (batch, - 1). \n",
    "    output of f() should be same dimension as x. \n",
    "\"\"\"\n",
    "def newton(f, x0):\n",
    "    print(\"newton\")\n",
    "    x = x0\n",
    "    tolerance = torch.Tensor((1e-5,))\n",
    "    while(True):\n",
    "        gradients = b_grad(f, x, x0)\n",
    "        print(gradients.size())\n",
    "        sx, x = x, x - bmm(inv(gradients), f(x, x0))\n",
    "        if(torch.norm(x - sx, p=2) < tolerance): break\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_v(x):\n",
    "    return x.view(x.size()[0], -1, 1)\n",
    "def to_m(x):\n",
    "    return x.view(x.size()[0], 1, int(x.size()[1] ** 0.5), int(x.size()[1] ** 0.5))\n",
    "\n",
    "def euler_backwards(f, x0, t0, t1, N, net_params):\n",
    "    print(\"euler_backwards\")\n",
    "    h = (t1 - t0) / float(N) # calculate step size\n",
    "    x, t = x0, t0\n",
    "    for i in range(N):\n",
    "        x = newton(lambda z, x0: z - f(z, t, net_params) - x0.view(z.size()), x)\n",
    "        t = t + h\n",
    "    return to_m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"lr\":  0.01,\n",
    "    \"n_epochs\": 1,\n",
    "    \"momentum\": 0.5,\n",
    "}\n",
    "\n",
    "solver_params = {\n",
    "    \"t0\": 0,\n",
    "    \"t1\": 3,\n",
    "    \"N\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "euler_backwards\n",
      "newton\n",
      "b_grad\n",
      "0\n",
      "torch.Size([1, 784, 784])\n",
      "b_grad\n",
      "0\n",
      "torch.Size([1, 784, 784])\n",
      "b_grad\n",
      "0\n",
      "torch.Size([1, 784, 784])\n",
      "b_grad\n",
      "0\n",
      "torch.Size([1, 784, 784])\n",
      "newton\n",
      "b_grad\n",
      "0\n",
      "torch.Size([1, 784, 784])\n",
      "b_grad\n",
      "0\n",
      "torch.Size([1, 784, 784])\n",
      "b_grad\n",
      "0\n",
      "torch.Size([1, 784, 784])\n",
      "output torch.Size([1, 10])\n",
      "backward\n",
      "torch.Size([1, 784, 1])\n",
      "flattening\n",
      "s0 torch.Size([1, 1587])\n",
      "integrating backwards dynamics\n",
      "torch.Size([1, 1587])\n",
      "euler_backwards\n",
      "newton\n",
      "b_grad\n",
      "0\n",
      "aug_dynamics\n",
      "unflattening\n",
      "flattening\n",
      "finished aug_dynamics\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-259-b8a7c6b691fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mTestNetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mODENet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meuler_backwards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBlock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTestNetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTestNetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-253-f96891e98845>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, train_loader, test_loader, hyperparameters)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \"\"\"\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TestNetwork = ODENet(euler_backwards,Block, solver_params)\n",
    "losses = train(TestNetwork, train_loader, test_loader, hyperparameters)\n",
    "torch.save(TestNetwork.state_dict(), \"test.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1684999465942383\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-250-3f0c3b9b3d87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mdxdt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mderivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-250-3f0c3b9b3d87>\u001b[0m in \u001b[0;36mdxdt\u001b[0;34m(f, x)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdxdt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjacobian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdzdt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.8/site-packages/torch/autograd/functional.py\u001b[0m in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict)\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0mjac_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnelement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             vj = _autograd_grad((out.reshape(-1)[j],), inputs,\n\u001b[0m\u001b[1;32m    438\u001b[0m                                 retain_graph=True, create_graph=create_graph)\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.8/site-packages/torch/autograd/functional.py\u001b[0m in \u001b[0;36m_autograd_grad\u001b[0;34m(outputs, inputs, grad_outputs, create_graph, retain_graph)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         return torch.autograd.grad(new_outputs, inputs, new_grad_outputs, allow_unused=True,\n\u001b[0m\u001b[1;32m    146\u001b[0m                                    create_graph=create_graph, retain_graph=retain_graph)\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m     return Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         inputs, allow_unused)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def square(x):\n",
    "    return x**2\n",
    "\n",
    "def derivative(f, x):\n",
    "    return jacobian(f, x)\n",
    "\n",
    "def dxdt(f, x):\n",
    "    return torch.autograd.functional.jacobian(f, x)\n",
    "\n",
    "def dzdt(f, z):\n",
    "    return jacobian(f, z)\n",
    "\n",
    "y = torch.rand((1,200), requires_grad=True)\n",
    "\n",
    "start = time.time()\n",
    "for i in range(0, 1):\n",
    "    dzdt(lambda x: derivative(square, x), y).view(1,-1)\n",
    "print(time.time() - start)\n",
    "\n",
    "start = time.time()\n",
    "for i in range(0, 1):\n",
    "    dxdt(lambda x: derivative(square, x), y).view(1,-1)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
